{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6803b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "json_files = [pos_json for pos_json in os.listdir(\".\") if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10fefdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175234\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dfs = []\n",
    "for f in json_files:\n",
    "    dfs.append(pd.read_json(path_or_buf=f, lines=True))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acab54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/shy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/shy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4573278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "902016ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, MiniBatchNMF, LatentDirichletAllocation\n",
    "#import sklearn.decomposition\n",
    "\n",
    "preprocessing_samples = list(filter(lambda x: isinstance(x, str), df['reviewText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175060it [00:00, 1964955.80it/s]\n",
      "175060it [00:00, 222011.16it/s]\n",
      "55429it [01:49, 496.51it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#preprocessing\n",
    "def lowercasing(data_samples):\n",
    "  for idx, sample in tqdm(enumerate(data_samples)):\n",
    "    data_samples[idx] = sample.lower()\n",
    "  return data_samples\n",
    "\n",
    "def punctuation_removal(data_samples):\n",
    "#non-exhaustive; not sure if we want to treat punctuation as significant\n",
    "#doesn't remove punctuation from inside words\n",
    "  for i, sample in tqdm(enumerate(data_samples)):\n",
    "    _sample = sample.split()\n",
    "    for j, word in enumerate(_sample):\n",
    "      _sample[j] = word.strip(\" .!?@#&():;,'\\/\\\\\")\n",
    "    sample = \" \".join(_sample)\n",
    "    data_samples[i] = sample\n",
    "  return data_samples\n",
    "\n",
    "#credit to Selva Prabhakaran\n",
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize(data_samples):\n",
    "  wnl = WordNetLemmatizer()\n",
    "  for i, sample in tqdm(enumerate(data_samples)):\n",
    "      _sample = sample.split()\n",
    "      for j, word in enumerate(_sample):\n",
    "        tag = get_wordnet_pos(word)\n",
    "        _sample[j] = wnl.lemmatize(word,tag)\n",
    "      data_samples[i] = \" \".join(_sample)\n",
    "  return data_samples\n",
    "\n",
    "preprocessing_samples = lowercasing(preprocessing_samples)\n",
    "preprocessing_samples = punctuation_removal(preprocessing_samples)\n",
    "preprocessing_samples = lemmatize(preprocessing_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cbb9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "batch_size = 128\n",
    "init = \"nndsvda\"\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.01, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "tf = tf_vectorizer.fit_transform(preprocessing_samples)\n",
    "tf_vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c33763",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['asin'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product_id = 'B009MA34NY'\n",
    "if product_id:\n",
    "    data_samples = list(filter(lambda x: isinstance(x, str), df[df['asin'] == product_id]['reviewText']))\n",
    "else:\n",
    "    data_samples = list(map(lambda x: x['text'], reviews))[:n_samples]\n",
    "\n",
    "\n",
    "for s in data_samples:\n",
    "\n",
    "    if not isinstance(s,str):\n",
    "        print(s)\n",
    "\n",
    "print(len(data_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = lowercasing(data_samples)\n",
    "data_samples = punctuation_removal(data_samples)\n",
    "data_samples = lemmatize(data_samples)\n",
    "documents = tf_vectorizer.transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "lda.fit(documents)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0503e",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29776280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(raw_reviews):\n",
    "    if len(sample) > 300 and len(sample) < 700:\n",
    "        print(i,\":\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862a006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "raw_reviews = list(filter(lambda x: isinstance(x, str), df[df['asin'] == product_id]['reviewText']))\n",
    "raw_sentences = sent_tokenize(raw_reviews[337])\n",
    "\n",
    "processed_sentences = raw_sentences[:]\n",
    "processed_sentences = lowercasing(processed_sentences)\n",
    "processed_sentences = punctuation_removal(processed_sentences)\n",
    "processed_sentences = lemmatize(processed_sentences)\n",
    "\n",
    "print(raw_sentences)\n",
    "processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "topic_words = []\n",
    "for topic in lda.components_:\n",
    "    top_features_ind = topic.argsort()[: -10- 1 : -1]\n",
    "    topic_words.append([feature_names[i] for i in top_features_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# TODO: for now compound (a composite score) will suffice. Neutrality (neu) might suggest highly informational content.\n",
    "for raw, processed in zip(raw_sentences, processed_sentences):\n",
    "    vs = analyzer.polarity_scores(raw)\n",
    "    probs = lda.transform(tf_vectorizer.transform([processed]))[0]\n",
    "    topic = probs.argmax()\n",
    "    if probs[topic] < 0.2:\n",
    "        print(\"{} \\n\\t overall: {:.2f} neutral: {:.2f}, No Topic\\n\".format(raw, vs['compound'], vs['neu']))\n",
    "    else:\n",
    "        print(\"{} \\n\\t overall: {:.2f} neutral: {:.2f}, Topic {}: {}\\n\".format(raw, vs['compound'], vs['neu'], topic+1, \", \".join(topic_words[topic])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c246030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "\n",
    "    processed_sentences = raw_sentences[:]\n",
    "    processed_sentences = lowercasing(processed_sentences)\n",
    "    processed_sentences = punctuation_removal(processed_sentences)\n",
    "    processed_sentences = lemmatize(processed_sentences)\n",
    "    \n",
    "\n",
    "    res = []\n",
    "    present_topics = set()\n",
    "    for raw, processed in zip(raw_sentences, processed_sentences):\n",
    "        vs = analyzer.polarity_scores(raw)\n",
    "        print(\"{} \\n\\t overall: {:.2f} neutral: {:.2f}\\n\".format(raw, vs['compound'], vs['neu']))\n",
    "\n",
    "\n",
    "        probs = lda.transform(tf_vectorizer.transform([processed]))[0]\n",
    "        topic = probs.argmax()\n",
    "        \n",
    "        res.append((raw, f\"Topic {topic+1} ({round(vs['compound'],2)})\"))\n",
    "        present_topics.add(topic)\n",
    "        \n",
    "    topics = {str(i+1): \", \".join(topic_words[i]) for i in sorted(list(present_topics))}\n",
    "    print(topics)\n",
    "    return [res, topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ef93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Glad I read the reviews and ordered a half size too big. These are light weight. I worked out in them last night and had a great work out. They are very comfortable. I would recommend these to anyone. I am a Beach Body Coach and these are now my new favorite shoes to work out in. I feel like I am walking in slippers when I wear these shoes. They are so comfortable. I LOVE them so much. I never buy myself anything nice and for Christmas got an Amazon gift card and used it to buy myself these sneakers and they are so awesome. I really do love them and have been telling all my friends about them. GET THESE SHOES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1215fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentiment_vals = np.linspace(-1.0, 1.0, num=201)\n",
    "color_map = {}\n",
    "\n",
    "colors = {1: \"red\", 2: \"orange\", 3: \"lime\", 4: \"pink\", 5: \"brown\", 6: \"green\", 7: \"purple\", 8: \"blue\", 9: \"cyan\", 10: \"yellow\"}\n",
    "\n",
    "for i, color in colors.items():\n",
    "    color_map.update({f\"Topic {i} ({round(val,2)})\": color for val in sentiment_vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ce98c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from gradio.components import Textbox, HighlightedText, JSON\n",
    "\n",
    "gr.Interface(fn=predict, \n",
    "             inputs=Textbox(placeholder=\"Enter review here...\", lines=5), \n",
    "             outputs=[HighlightedText().style(color_map=color_map), JSON()]) \\\n",
    "    .launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c991a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c307bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
