{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88efbcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ['Digital_Music_5.json']\n",
      "\n",
      "filename             samples\n",
      "Digital_Music_5      169781 \n",
      "\n",
      "Data loaded, 169781 total samples.\n"
     ]
    }
   ],
   "source": [
    "from data import generate_dataframe\n",
    "\n",
    "df = generate_dataframe([\"Digital_Music_5.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99fa861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/shy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/shy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying lowercase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169623it [00:00, 2031780.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying remove_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169623it [00:00, 236588.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169623it [04:54, 575.58it/s] \n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocess_samples\n",
    "from utils import get_product_reviews\n",
    "\n",
    "raw_corpus_samples = list(filter(lambda x: isinstance(x, str), df['reviewText']))\n",
    "corpus_samples = preprocess_samples(raw_corpus_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1405466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying lowercase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying remove_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying lemmatize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "product_id = 'B009MA34NY'\n",
    "raw_product_samples = get_product_reviews(df, product_id)\n",
    "product_samples = preprocess_samples(raw_product_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fade73ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/shy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 0 elements, new values have 1 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mn_grams\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bigrams \u001b[38;5;241m=\u001b[39m \u001b[43mn_grams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_bigrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trigrams \u001b[38;5;241m=\u001b[39m n_grams\u001b[38;5;241m.\u001b[39mget_trigrams(product_samples, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m17\u001b[39m)\n\u001b[1;32m      4\u001b[0m product_samples \u001b[38;5;241m=\u001b[39m n_grams\u001b[38;5;241m.\u001b[39mreplace_n_grams(product_samples, bigrams, trigrams)\n",
      "File \u001b[0;32m~/Projects/CSCI5525/project/n_grams.py:23\u001b[0m, in \u001b[0;36mget_bigrams\u001b[0;34m(data_samples, occur_rate, pmi_threshold)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bigrams\u001b[39m(data_samples, occur_rate, pmi_threshold):\n\u001b[1;32m     22\u001b[0m     ds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_samples)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m     bigram_measures \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mcollocations\u001b[38;5;241m.\u001b[39mBigramAssocMeasures()\n\u001b[1;32m     26\u001b[0m     finder \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mcollocations\u001b[38;5;241m.\u001b[39mBigramCollocationFinder\u001b[38;5;241m.\u001b[39mfrom_documents([r\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mreview])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pandas/core/generic.py:5915\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5913\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   5914\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 5915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   5917\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pandas/_libs/properties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pandas/core/generic.py:823\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: \u001b[38;5;28mint\u001b[39m, labels: AnyArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pandas/core/internals/managers.py:227\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: \u001b[38;5;28mint\u001b[39m, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pandas/core/internals/base.py:70\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 1 elements"
     ]
    }
   ],
   "source": [
    "import n_grams\n",
    "bigrams = n_grams.get_bigrams(product_samples, 10, 6)\n",
    "trigrams = n_grams.get_trigrams(product_samples, 2, 17)\n",
    "product_samples = n_grams.replace_n_grams(product_samples, bigrams, trigrams)\n",
    "\n",
    "print(bigrams[:10])\n",
    "print(trigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12919a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_metrics as em\n",
    "freqs = em.get_word_freqs(product_samples)\n",
    "bi_freqs = em.get_bi_freqs(product_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d9539",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import train\n",
    "tf_vectorizer = train.get_tf_vectorizer()\n",
    "tf_vectorizer.set_params(tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07663c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8925ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_documents = tf_vectorizer.fit_transform(product_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8b43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92434485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "# Compute topic coherence\n",
    "n_components = range(1,10)\n",
    "coherences = []\n",
    "num_docs = len(product_samples)\n",
    "\n",
    "for num in n_components:\n",
    "  lda = train.get_lda(samples=product_documents, n_components=num)\n",
    "  tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "  coherence = em.avg_umass(lda, tf_feature_names, train.n_top_words, freqs, bi_freqs, num_docs)[1]\n",
    "  coherences.append(coherence)\n",
    "\n",
    "for i, c in enumerate(coherences):\n",
    "  print(\"Number of topics: \", n_components[i])\n",
    "  print(\"Average UMass coherence: \", c)\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfcdeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import plot_top_words\n",
    "coherences = [abs(c) for c in coherences]\n",
    "n_components = n_components[coherences.index(min(coherences))]\n",
    "\n",
    "lda = train.get_lda(samples=product_documents, n_components=n_components)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, train.n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6fed1",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Topic numbering in histogram plots and pyLDAvis are not the same\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "topic_data = pyLDAvis.sklearn.prepare(lda, product_documents, tf_vectorizer)\n",
    "pyLDAvis.display(topic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ec6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities, x, y = em.jaccard(lda,tf_feature_names,30)\n",
    "em.print_jaccard(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b410e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['asin'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351fa8ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import preprocess_sample\n",
    "\n",
    "raw_reviews = list(filter(lambda x: isinstance(x, str), df[df['asin'] == product_id]['reviewText']))\n",
    "processed_sentences, raw_sentences = preprocess_sample(raw_reviews[337], get_raw=True)\n",
    "\n",
    "processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e98fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(raw_reviews):\n",
    "    if len(sample) > 300 and len(sample) < 700:\n",
    "        print(i,\":\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaac982",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "topic_words = []\n",
    "for topic in lda.components_:\n",
    "    top_features_ind = topic.argsort()[: -10- 1 : -1]\n",
    "    topic_words.append([feature_names[i] for i in top_features_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# TODO: for now compound (a composite score) will suffice. Neutrality (neu) might suggest highly informational content.\n",
    "for raw, processed in zip(raw_sentences, processed_sentences):\n",
    "    vs = analyzer.polarity_scores(raw)\n",
    "    probs = lda.transform(tf_vectorizer.transform([processed]))[0]\n",
    "    topic = probs.argmax()\n",
    "    if probs[topic] < 0.2:\n",
    "        print(\"{} \\n\\t overall: {:.2f} neutral: {:.2f}, No Topic\\n\".format(raw, vs['compound'], vs['neu']))\n",
    "    else:\n",
    "        print(\"{} \\n\\t overall: {:.2f} neutral: {:.2f}, Topic {}: {}\\n\".format(raw, vs['compound'], vs['neu'], topic+1, \", \".join(topic_words[topic])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69909131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "\n",
    "    processed_sentences = raw_sentences[:]\n",
    "    processed_sentences = lowercasing(processed_sentences)\n",
    "    processed_sentences = punctuation_removal(processed_sentences)\n",
    "    processed_sentences = lemmatize(processed_sentences)\n",
    "    \n",
    "\n",
    "    res = []\n",
    "    present_topics = set()\n",
    "    for raw, processed in zip(raw_sentences, processed_sentences):\n",
    "        vs = analyzer.polarity_scores(raw)\n",
    "        print(\"{} \\n\\t overall: {:.2f} neutral: {:.2f}\\n\".format(raw, vs['compound'], vs['neu']))\n",
    "\n",
    "\n",
    "        probs = lda.transform(tf_vectorizer.transform([processed]))[0]\n",
    "        topic = probs.argmax()\n",
    "        \n",
    "        res.append((raw, f\"Topic {topic+1} ({round(vs['compound'],2)})\"))\n",
    "        present_topics.add(topic)\n",
    "        \n",
    "    topics = {str(i+1): \", \".join(topic_words[i]) for i in sorted(list(present_topics))}\n",
    "    print(topics)\n",
    "    return [res, topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentiment_vals = np.linspace(-1.0, 1.0, num=201)\n",
    "color_map = {}\n",
    "\n",
    "colors = {1: \"red\", 2: \"orange\", 3: \"lime\", 4: \"pink\", 5: \"brown\", 6: \"green\", 7: \"purple\", 8: \"blue\", 9: \"cyan\", 10: \"yellow\"}\n",
    "\n",
    "for i, color in colors.items():\n",
    "    color_map.update({f\"Topic {i} ({round(val,2)})\": color for val in sentiment_vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94553c25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from gradio.components import Textbox, HighlightedText, JSON\n",
    "\n",
    "gr.Interface(fn=predict, \n",
    "             inputs=Textbox(placeholder=\"Enter review here...\", lines=5), \n",
    "             outputs=[HighlightedText().style(color_map=color_map), JSON()],\n",
    "             examples=[\n",
    "        [\"Good indoor training shoes for running on treadmill, doing lunges and regular exercises at the gym. These are very flexible, light weight and comfortable. Grip is okay - sticky rubber is used only at the edges of heel and toe areas so I slipped a little when I worked on cable machines, resistance band, etc. on un-carpeted floor.  I would emphasize that if you do lifting as a part of your everyday routine workout I would not recommend them because mine (cushion) lasted only for six months and this is the reason I gave three stars. Other than that, I liked them!\"],\n",
    "        [\"I've had these shoes for about a week now and have so far enjoyed using them. Considering the fact that I have wide feet, the shoes are slightly tight. However, it doesn't feel uncomfortable nor does it bothers me as I use them throughout my workouts. I know some people personally like when the shoes are a bit tighter or a bit looser so it's all in personal preference.\"],\n",
    "        [\"The picture makes the shoe look like it has a \\\"boxier\\\" toe rather than the \\\"pointier\\\" toe that it actually has. I have wider feet and generally need to buy a size or half size longer to get a comfortable width (in any brand of shoe). I was shooting for a rounder, broader toe design which is more comfortable for me, and I feel that the pictures of this shoe didn't accurately depict what I received, in that one detail. Otherwise, \\\"the shoe fits\\\" So I am wearing it.\"]\n",
    "    ],\n",
    ") \\\n",
    "    .launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6560f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a127e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
